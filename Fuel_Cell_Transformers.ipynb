{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:44:42.821349Z",
     "start_time": "2025-02-27T15:44:42.811817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sympy.codegen import Print\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Custom Truncated Normal Initialization\n",
    "# ------------------------------\n",
    "def truncated_normal_(tensor, mean=0.0, std=0.2, a=-2.0, b=2.0):\n",
    "    \"\"\"\n",
    "    Custom truncated normal initialization.\n",
    "    This method ensures values stay within the range [a, b].\n",
    "    \"\"\"\n",
    "    lower, upper = (a - mean) / std, (b - mean) / std\n",
    "    tensor.data = torch.distributions.Normal(mean, std).rsample(tensor.shape)\n",
    "    tensor.data = torch.clip(tensor.data, min=a, max=b)\n",
    "    return tensor\n",
    "\n",
    "# ------------------------------\n",
    "# Input Embedding Module (CNN-Based)\n",
    "# ------------------------------\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=256, kernel_sizes=[4, 3], strides=[2, 2]):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dim, embed_dim, kernel_sizes[0], stride=strides[0])\n",
    "        self.bn1 = nn.BatchNorm1d(embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_sizes[1], stride=strides[1])\n",
    "        self.bn2 = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(truncated_normal_(torch.empty(1, 1, embed_dim)))\n",
    "        self.pos_embedding = nn.Parameter(truncated_normal_(torch.empty(1, embed_dim + 1, embed_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, channels, time_steps)\n",
    "        Output: (batch_size, seq_len+1, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # Reshape for transformer (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.pos_embedding[:, :x.shape[1], :]\n",
    "\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# Multi-Head Self-Attention\n",
    "# ------------------------------\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# ------------------------------\n",
    "# Feed-Forward Network (MLP Block)\n",
    "# ------------------------------\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim, ffn_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(ffn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.fc2(self.gelu(self.fc1(x))))\n",
    "\n",
    "# ------------------------------\n",
    "# DropPath (Stochastic Depth)\n",
    "# ------------------------------\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        random_tensor = keep_prob + torch.rand(x.shape, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "# ------------------------------\n",
    "# Transformer Block\n",
    "# ------------------------------\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=16, ffn_dim=1024, drop_path_rate=0.1, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, attn_dropout)\n",
    "        self.drop_path1 = DropPath(drop_path_rate)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForwardNetwork(embed_dim, ffn_dim)\n",
    "        self.drop_path2 = DropPath(drop_path_rate)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.drop_path1(self.attn(self.norm1(x), mask))\n",
    "        x = x + self.drop_path2(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# Transformer Encoder\n",
    "# ------------------------------\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_blocks=4, num_heads=16, ffn_dim=1024, drop_path_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ffn_dim, drop_path_rate)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# SOH-TEC Model\n",
    "# ------------------------------\n",
    "class SOHTEC(nn.Module):\n",
    "    def __init__(self, input_dim=5, embed_dim=256, num_blocks=4, num_heads=16, ffn_dim=1024, drop_path_rate=0.1):\n",
    "        super(SOHTEC, self).__init__()\n",
    "\n",
    "        self.embedding = InputEmbedding(input_dim, embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim, num_blocks, num_heads, ffn_dim, drop_path_rate)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim // 2, 1)  # Regression output for SOH estimation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.mlp_head(x[:, 0])  # Using CLS token for SOH prediction\n"
   ],
   "id": "219097e1e60f2fd5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "857a7f62",
   "metadata": {},
   "source": [
    "# Fuel Cell"
   ]
  },
  {
   "cell_type": "code",
   "id": "65a985ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T16:01:57.804952Z",
     "start_time": "2025-02-27T16:01:57.556670Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DATA =====================================================================================================\n",
    "\n",
    "for hours in [100, 10, 0.25]:\n",
    "\n",
    "    print(f'For {hours} hour splits:\\n')\n",
    "\n",
    "    train_df = pd.read_csv(\"C:/Users/serha/PycharmProjects/AISolutions/PDM/battery_transformer/data/fuel_cell/FC1_train_val_filtered.csv\")\n",
    "\n",
    "    features = ['I (A)']\n",
    "    X = train_df[features].values\n",
    "\n",
    "    y = train_df['Utot (V)'].values if 'Utot (V)' in train_df.columns else None\n",
    "    max_y = max(y)\n",
    "\n",
    "    time = train_df['Time (h)'].values\n",
    "\n",
    "    scaler_data = StandardScaler()\n",
    "    X = scaler_data.fit_transform(X)\n",
    "    y = y / max_y\n",
    "\n",
    "    SEQ_LEN = 100\n",
    "    NUM_FEATURES = len(features)\n",
    "\n",
    "    def create_variable_length_sequences(X, y, time, time_window=10.0, max_length=400):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        current_sequence = []\n",
    "        current_target = []\n",
    "        start_time = time[0]\n",
    "\n",
    "        for i in range(len(time)):\n",
    "            if time[i] - start_time > time_window:\n",
    "                if len(current_sequence) >= max_length:  # Ensure uniform length\n",
    "                    sequences.append(np.array(current_sequence[:max_length], dtype=np.float32))\n",
    "                    targets.append(np.array(current_target[:max_length], dtype=np.float32))\n",
    "                current_sequence = []\n",
    "                current_target = []\n",
    "                start_time = time[i]\n",
    "\n",
    "            current_sequence.append(X[i])\n",
    "            if y is not None:\n",
    "                current_target.append(y[i])\n",
    "\n",
    "        if len(current_sequence) >= max_length:\n",
    "            sequences.append(np.array(current_sequence[:max_length], dtype=np.float32))\n",
    "            targets.append(np.array(current_target[:max_length], dtype=np.float32))\n",
    "\n",
    "        return sequences, targets\n",
    "\n",
    "\n",
    "    X_var_len, y_var_len = create_variable_length_sequences(X, y, time, hours)\n",
    "\n",
    "    X_seq = X_var_len\n",
    "\n",
    "    train_size = int(0.8 * len(X_seq))\n",
    "    val_size = int(0.1 * len(X_seq))\n",
    "    test_size = len(X_seq) - train_size - val_size\n",
    "\n",
    "    X_train, y_train = X_var_len[:train_size], y_var_len[:train_size]\n",
    "    X_val, y_val = X_var_len[train_size:train_size+val_size], y_var_len[train_size:train_size+val_size]\n",
    "    X_test, y_test = X_var_len[train_size+val_size:], y_var_len[train_size+val_size:]\n",
    "\n",
    "    class VariableLengthSOHDataset(Dataset):\n",
    "        def __init__(self, X_list, y_list):\n",
    "            self.X_list = [torch.tensor(x, dtype=torch.float32).transpose(0, 1) for x in X_list]  # (1, time_steps)\n",
    "            self.y_list = [torch.tensor(y, dtype=torch.float32) for y in y_list]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X_list)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X_list[idx], self.y_list[idx]\n",
    "\n",
    "\n",
    "    # Custom Collate Function for Padding\n",
    "    def collate_fn(batch):\n",
    "        X_batch, y_batch = zip(*batch)\n",
    "\n",
    "        print(\"Batch Sizes Before Padding:\")\n",
    "        for i, x in enumerate(X_batch):\n",
    "            print(f\"Sample {i}: {x.shape}\")  # Should be (time_steps, 1)\n",
    "\n",
    "        X_padded = pad_sequence(X_batch, batch_first=True, padding_value=0.0)  # (batch, max_time_steps, 1)\n",
    "        y_padded = pad_sequence(y_batch, batch_first=True, padding_value=-1)\n",
    "\n",
    "        print(f\"Padded Batch Shape: {X_padded.shape}\")  # Should be (batch, max_time_steps, 1)\n",
    "        return X_padded, y_padded\n",
    "\n",
    "\n",
    "    train_dataset = VariableLengthSOHDataset(X_train, y_train)\n",
    "    val_dataset = VariableLengthSOHDataset(X_val, y_val)\n",
    "    test_dataset = VariableLengthSOHDataset(X_test, y_test)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    # MODEL ================================================================================================\n",
    "\n",
    "    model = SOHTEC(input_dim=NUM_FEATURES, embed_dim=256).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "\n",
    "    def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X).squeeze()\n",
    "\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    outputs = model(batch_X).squeeze()\n",
    "                    val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), \"best_soh_tec_model.pth\")\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20)\n",
    "\n",
    "    # TESTS ================================================================================================    \n",
    "\n",
    "    def evaluate_model(model, test_loader):\n",
    "        model.load_state_dict(torch.load(\"best_soh_tec_model.pth\"))\n",
    "        model.eval()\n",
    "\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X).squeeze()\n",
    "\n",
    "                all_preds.append(outputs.cpu().numpy())\n",
    "                all_targets.append(batch_y.cpu().numpy())\n",
    "\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "        mae = mean_absolute_error(all_targets, all_preds)\n",
    "        r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "        print(f\"Test RMSE: {rmse:.4f}\")\n",
    "        print(f\"Test MAE: {mae:.4f}\")\n",
    "        print(f\"Test RÂ²: {r2:.4f}\")\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_model(model, test_loader)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 100 hour splits:\n",
      "\n",
      "Batch Sizes Before Padding:\n",
      "Sample 0: torch.Size([1, 400])\n",
      "Sample 1: torch.Size([1, 400])\n",
      "Sample 2: torch.Size([1, 400])\n",
      "Sample 3: torch.Size([1, 400])\n",
      "Sample 4: torch.Size([1, 400])\n",
      "Sample 5: torch.Size([1, 400])\n",
      "Sample 6: torch.Size([1, 400])\n",
      "Padded Batch Shape: torch.Size([7, 1, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serha\\PycharmProjects\\AISolutions\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([7, 400])) that is different to the input size (torch.Size([7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (400) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 158\u001B[0m\n\u001B[0;32m    155\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_soh_tec_model.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m--> 158\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;66;03m# TESTS ================================================================================================    \u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mevaluate_model\u001B[39m(model, test_loader):\n",
      "Cell \u001B[1;32mIn[24], line 133\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[0;32m    130\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    131\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(batch_X)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m--> 133\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    135\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\PycharmProjects\\AISolutions\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\AISolutions\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\AISolutions\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 610\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\AISolutions\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3884\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction, weight)\u001B[0m\n\u001B[0;32m   3881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3882\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3884\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3886\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3887\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m weight\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize():\n",
      "File \u001B[1;32m~\\PycharmProjects\\AISolutions\\.venv\\Lib\\site-packages\\torch\\functional.py:76\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[1;34m(*tensors)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (7) must match the size of tensor b (400) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb59f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
